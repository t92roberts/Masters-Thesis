\chapter{Future Work}
\label{ChapterFutureWork}

This chapter will summarise potential extensions that could be implemented in future projects to improve the effectiveness and efficiency of the work. It will discuss both the modelling of the problem and the implementation of the local search metaheuristic.

\section{Model} \label{sec:future_work_problem_modelling}

\subsection{Developer Competencies}
In the ideal agile development team, developers have the complete set of skills required to deliver any story in the backlog, and so any set of stories can be assigned to the team's sprint backlog. In reality, many teams are compromised of developers with different competencies. Therefore, simply assigning the highest-value stories to the sprint backlog is not always realistic. To illustrate this, if a team consisted of a front-end developer and a back-end developer, and the user stories relating to the front-end development have been prioritised as high value by the Product Owner, then the optimal sprint plan is to assign only front-end tasks. Clearly, this might deliver the most business value but it is not an efficient way to utilise the development team. An alternative model could be rather than assigning tasks to the team backlog, each development team member has their own personal sprint backlog which is to be optimised. The team's sprint backlog is then simply the union of each team member's personal sprint backlog.

\subsection{Additional User Story Variables}
The model in this thesis covers only the core inputs needed to create a road map. Variables like those included in \citet{golfarelli2012sprint} such as uncertainty, affinity, criticality may create the same risk of non-delivery as mishandling dependencies or sprint capacities. As \citet{wake_2003} notes, the size of a story is a function of how well it is understood, and so the uncertainty of a story can impact the reliability of its estimation. Affinity could create more coherent plans so that stories that are naturally related are worked on in the same or nearby sprints. Affinity could also be modelled as a relationship between stories and developers since it makes sense that the developers work on tasks that are similar to ones they have previously worked on. 

Additional variables could also be interesting to model. If a purely technical user story will not deliver much business value itself but will enable delivery of more business-valuable stories, 'IT Value' could give a way for the development team to prioritise these IT tasks. In many industries, there may be regulatory requirements that do not deliver much customer or IT value but still require significant development time, and a 'Regulatory Value' variable could account for this.

\subsection{Fixed Assignments}
While agile planning encourages great flexibility in the order that stories are worked on, sometimes a story (or set of stories) must be delivered on or before a certain date. If a legal or regulatory requirement has a hard deadline, the development team must deliver the required functionality on time. These tasks must therefore dominate the placement of other stories even if they have lower business value. An additional set of constraints could be added to the model to accommodate for fixing the placement of stories on or before certain sprints.

\subsection{Scaled Agile}
The focus of this thesis has been on a single agile team but the real difficulty in dependency management and creating realistic plans comes when multiple teams are working together. This not only increases the size of the problem but also the complexity when teams have dependencies in each other's backlogs. The Scaled Agile Framework (SAFe) for Lean Enterprises \citep{scaled_agile_inc_2018} offers a framework for orchestrating multiple scrum teams who are producing software towards common, high-level objectives. Program Increment Planning is a significant event were the teams essentially perform their sprint planning events all together and plan for the next 4 or 5 sprints. It requires careful planning and coordination between the teams to ensure that dependencies and risks are managed and realistic Program Increment objectives can be committed to. An algorithm that can automatically create a good plan could dramatically increase the efficiency of these events by removing the need to do manual scheduling across many scrum teams. The model would need to be extended to include the concepts associated with SAFe such as PI Objectives, Strategic Themes, Solution Trains, Portfolios, and new stakeholders such as Release Train Engineers, Solution Managers, and Enteprise Architects. Prioritisation of the high- level objectives could flow down and influence the planning decisions of the individual scrum teams to ensure that a road map not only creates value to one team but collectively adds value to the whole portfolio in an optimal way.

\section{Adaptive Metaheuristics}

A possible bottleneck of the local search's implementation is that many of the metaheuristic parameters are chosen a priori. While much effort was put into experimentally finding good parameters for the given sets of test data, it is very difficult to choose values that work well for all problems. Therefore, implementing adaptive metaheuristics that can adjust their parameters on the fly during the search may further reduce the gap in value between the local search and the optimal search results.

\subsection{Tabu Search}
The tenure of moves and size of the tabu list have a significant impact on the effectiveness of tabu search. More robust variations of tabu search suggest randomly adjusting the tenure and size rather than fixing them beforehand, but parameters such as how much variation there should be in the random values must still be chosen. Reactive Tabu Search \citep{battiti1994reactive} designs the tabu list so that the size of the list is learned automatically during the search. It tracks how often each move occurs and if moves are being frequently repeated, the size of the list is increased. When regions of the search space that do not need such a large list are being explored, the list size is slowly reduced.

\subsection{Solution Destruction}

\subsubsection{Adaptive Level of Destruction}
Deciding how much of the solution to remove can be hard. On one hand, the search could remove all related assignments (as briefly discussed in section \ref{destroy}) but this eliminates the control over how much of the solution is destroyed. On the other hand, the search could remove a certain number of assignments but the exact number can change during the course of the search rather than being fixed beforehand. The general idea could be that when exploration is needed (for example to break out of a local optimum), the number of stories removed is increased to generate neighbours that are 'further away' from the current solution. Conversely, if higher-value solutions are being discovered, the search can reduce the number of stories removed to focus on intensification and polishing the current solution.

\subsubsection{Adaptive Destruction and Repair Methods}
Two ways of destroying a solution were used in this work: random and radial. These were applied alternately to give some variety in how the search generated neighbours. However, one of these methods may be more effective in progressing the search forward in different problems or in different stages of the search. It could therefore be interesting if the search tracks how well each destroy or repair heuristic performs. Intuitively, heuristics that lead the search to high quality solutions should be used more often. \citet{ropke2006adaptive} proposes a variation of Large Neighbourhood Search named \emph{Adaptive Large Neighbourhood Search} that uses a number of competing sub-heuristics which are used with a frequency corresponding to their historical performance. The idea is to keep track of a score for each heuristic and its score is increased if the last destroy-repair operation either: (1) finds a new global optimum (this is of course desirable), (2) discovers an unvisited solution that improves the current solution (it was able to find a better neighbour which had not been seen before), or (3) discovers an unvisited solution that does not improve the current solution (it was able to diversify the search to a new area, even if it didn't improve the current/global solution). This dynamic approach allows the search to favour good heuristics but also allows it to 'change its mind' about which it uses during the search as the scores are adjusted.

\subsection{Solution Repair}
Using a greedy heuristic to insert stories into a partial solution may be fast but it doesn't guarantee to create the best solution possible after repair. If the number of assignments removed is relatively small, a complete algorithm could be used as the mechanism to repair. This could reduce the amount of time spent exploring lower value solutions that are only visited because the greedy heuristic was not able to construct the best solution for a given partial solution.

\section{Code Improvements}

\subsection{Parallelisation}
Local search is inherently iterative - the next iteration depends on the result on the previous one. However, parts of the code have the potential to be parallelised to increase the amount of CPU time that the search uses to find the global optimum while possibly using less wall time.

As the size of the problem grows, the number of assignments that the constraint checking and objective function must examine grows. This can have a major influence on the overall execution time of the local search. However, simply checking the set of assignments does not require any kind of mutation so there would potentially be no race conditions if multiple threads read the assignments at the same time. In other words, the search does not need to wait for one story/sprint to have its constraints checked before processing the next. A potentially significant time boost could be gained by spawning a thread that checks the constraints for a single story/sprint (or a subset of stories/sprints) so that all are checked in parallel. Similarly, the objective function could sum the business value of each sprint in parallel and then sum the subtotals into a grand total when the threads are joined. When Tabu search is used, the local search must check every move against the tabu list. Again, reading the tabu list does not mutate it and so the code could check all moves in parallel at the same time.

Random restarts try to cover as much of the search space as possible by starting at different random positions and performing a local search. The best solution found over the multiple restarts is then returned as the final result. Instead of waiting for the first search to stagnate before starting the next one, several local searches could be spawned to traverse random parts of the search space at the same time. This draws parallels with Particle Swarm Optimisation but each thread does not necessarily need to interact with the others. Instead, a number of threads can be spawned that each runs the local search. When the threads are joined at the end, the best of the best results is returned.

\section{End-to-end Tool}
In the Model-View-Controller architectural design pattern \citep{pope1988cookbook}, applications are divided into 3 parts. The Model manages the data representation of the system, the View displays representations of the data to the user, and the Controller contains the 'business logic' that updates the view and the model. Looking at this thesis using the MVC pattern, the focus has mostly been on developing the controller. The usability of the work is minimal as little attention was paid to how the data is stored or how an agile team would view the data and interact with the results produced by the code.

\subsection{Model}
The 'static' data is stored as flat CSV files that are read in by the program. Where possible, efficient data structures are used to access the data needed by the local search - for example, stories and sprints are given an ID number and are placed in an array at the corresponding index so that they can be accessed in constant time if the ID is known. Also the tabu list is implemented using maps to allow fast look-ups of moves. However as mentioned in previous sections, the stories and dependency links can be seen as a directed, acyclic graph where nodes are stories and edges are dependency links. This model fits naturally with the concepts of a graph database and operations that must traverse the graph may benefit from representing the data in this way compared to the flat structure used in this project. Additionally, the extensions to the modelling of the problem covered in section \ref{sec:future_work_problem_modelling} also fit with a graph database if one uses different types of nodes to represent developers, skills etc that are connected by edges representing the relationships between them.

While storing the static data of the problem in a graph database may have its advantages in terms of looking up the constant values associated with stories and sprints, it could also be an interesting way to represent a solution in the local search. If one type of node represents a sprint and another represents a story, a directed edge from a story to a sprint could represent that the story is assigned to the sprint. The code to destroy a solution could be greatly simplified by replacing a C++ implementation of breadth-first search with a simple graph database query. As well as simplifying the code, it would decouple the 'business logic' from the model of the data by no longer requiring that the controller must know how to traverse the relationships between the static data in the model. The process of destroying and repairing a solution could therefore be to disconnect and reconnect stories from sprints in the graph database.

Furthermore, a hybrid of these two points may be an even more intuitive and powerful way to model the problem where stories are connected to sprints by an 'assigned to' edge, and stories are connected to each other by 'depends on' edges. The local search algorithm could then have a single model where it can look up static data, generate neighbours, and check the constraints.

An extension that goes hand in hand with using a database could be to implement some integration with popular commercial agile project management tools like JIRA so that a team can simply import their existing backlog into the model ready to be solved.

\subsection{View}
Since the test data was generated automatically, building a user-friendly way to create and interact with the backlog was low priority. If the implementation is to be used by a real agile team, some kind of user interface would be necessary so that they are able to view and edit the stories (and dependencies) and sprints. If a database is used to persist the data, this user interface could just be a standard rows and columns view that allows the user to edit the static data. The interface could also be a custom visualisation of the graph structure of the model which maY be a more intuitive way to work with it.