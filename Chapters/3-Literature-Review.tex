% Chapter Template

\chapter{Literature Review}
\label{ChapterLiteratureReview}

This chapter reviews how other research has tackled the ASPP using either complete algorithms or approximate algorithms.

\section{Related Work}

\citet{golfarelli2012sprint} identifies that agile methods show promising results in overcoming problems in traditional software engineering projects. It explains how while many commercial tools support generic management of the agile process, they do not provide a way of developing the optimal sprint plan in a project to develop a data warehouse. The paper formalises the ASPP as a multi-knapsack problem with constraints, where sprints are the knapsacks and user stories are the items. The problem is categorised as \enquote{resource-constrained with renewable resources (i.e., manpower) available on a period-by-period basis. As in the basic PERT/CPM model, finish-start precedences with zero time lag are considered and no preemption of activities is allowed.} It is then given an integer programming formulation and solved using the IBM ILOG CPLEX Optimiser. The model covers the critical variables needed to plan a sprint such as sprint capacity (defined by the development team's velocity), user story business value, and user story points. It also uses an additional set of constants (measured on an ordinal scale) to model the risks associated with a user story such as how 'critical' it is to the project as a whole and the 'uncertainty' of the estimations. Lastly, it models the 'affinity' of stories so that sets of related user stories can be identified. These non-critical variables are used to weight a user story's business value or story points so that risky stories, or stories whose siblings have been assigned to the same sprint, are perceived as more valuable to work on and are more likely to be assigned to a sprint plan. While the use case of this problem is in building a data warehouse, the model that was implemented is a general model of agile methodologies that can be applied to other domains. Their model was validated in two ways: effectiveness and efficiency. The effectiveness test was done by taking a real project that had already ended from a development team and using the product backlog as the input to the model. The model created the optimal sprint plan and this was compared to the actual sprint plan that the team had used. The team's plan was then compared to the generated plan to compare the amount of business value delivered in each sprint, the distribution of risk across sprints, and how dependencies and delays were handled. The overall results were positive as the leader of the team judged that the optimal sprint plan was an improvement over the team's own plan. Secondly, the efficiency of the model was tested by evaluating how long it took to solve problem instances of increasing size (in terms of number of stories) and complexity (in terms of number of dependencies). However, it is shown that as the size and complexity grows, the computation time grows exponentially due to an exponential increase in the search space. The authors note that solving instances of more than 100 stories, or instances with many dependencies, becomes too time-consuming. Therefore, this thesis will extend the model and explore using metaheuristics to find a better trade-off between time and the quality of solutions compared to the mathematical programming approaches like Branch and Bound used in \citet{golfarelli2012sprint}.

Even with a well organised backlog and a carefully designed sprint plan, it is common that some user stories are not delivered in the sprint that they were planned in. This inevitably has an impact on any stories that depend on it and on any lower-value stories that were planned for the next sprints but now need to be moved to make room for the higher-value spillover. Additionally, agile frameworks give the flexibility to add or remove stories at any point in a project's lifetime. Thus, the plan generated up front often needs to be adjusted. \citet{golfarelli2013multi} describes how a new sprint plan should accommodate for this spillover and allow new stories to be scheduled, but to minimise the disruption to the teams and external stakeholders, it is desirable that the new plan is close to the original one so that any committed due dates and resource allocations can be preserved as best as possible. The basic model given in \citet{golfarelli2012sprint} is extended with the idea of supporting smooth re-planning and is solved using the IBM ILOG CPLEX Optimiser. To begin with, a 'baseline' sprint plan is generated. When a new plan needs to be made, a 'minimum perturbation strategy' is used. This strategy is seen as a multi-objective optimisation problem that tries to maximise the value of the sprint plan while also maximising the similarity between the new sprint plan and the baseline sprint plan. The paper suggests that this can be done hierarchically where one objective dominates the other - that is, the sum of the business value is kept optimal while the similarity is maximised to the greatest extent possible without reducing the business value of the sprint plan. Alternatively, a simultaneous approach could use a weighted combination of the business value and the similarity to create a more complex objective function. Ultimately, the paper chooses the first approach since maximising the value of the new sprint plan always takes priority over the similarity to the baseline. The paper focuses mainly on how to generate new plans based on an existing plan using Branch and Bound but it does not explore other approaches that could solve the ASPP, whereas this thesis will explore new ways of solving the ASPP. Since the concept of smooth re-planning is mainly an extension of the objective function and the model, the new solving methods explored in this thesis could be applied in both the baseline planning and the re-planning phases.

\citet{brezovcnik2018scrum} tackles the Scrum task allocation problem using an algorithm inspired by nature called Particle Swarm Optimisation (PSO) \citep{eberhart1995particle}. This algorithm is inspired by the behaviour of groups of natural organisms that work together to find desirable areas of a space. If the organisms start from different random positions, the swarm is able to cover most of the total space and find the best area. This approach has been applied to optimisation problems where a set of candidate solutions is treated as a swarm of particles and each 'particle' carries out a local search and moves around its local space trying to find a better solution. Each candidate solution is encoded as a vector representing its position in the solution space and has a velocity that dictates the direction that it moves after each iteration. The swarm maintains the highest-value position found so far and the velocity of each particle is influenced by the position of its own local optimum and by the best known positions found by other particles. During an iteration, each particle updates the velocities of the other particles relative to the value of its best local solution. Particles are therefore attracted to high value solutions and over time, all of the particles hopefully converge on the global optimum. The advantage of PSO is that it combines exploration of the solution space with intensification of local solutions. However, as with other local search methods, the parameters chosen by the user can have a great impact on the quality of the results. For example, if all particles can communicate with each other, a particle that is close to the global optimum (but has not yet encountered it) may be attracted away from it and pulled towards a very good (but sub-optimal) solution found by another particle in another area. It can therefore give better results to limit the set of particles that each one communicates with so that they are not influenced too heavily by others. PSO is one of a plethora of approximation techniques and this thesis will explore other techniques such as large neighbourhood search, simulated annealing, and tabu search to find if they can also be applied to the ASPP.

\citet{greer2004software} proposes a method called EVOLVE that is based on a genetic algorithm. It optimally assigns a set of customer requirements to 'increments' according to the prioritisation made by the Product Owner, precedence constraints, and resource constraints. It aims to provide a way to conduct continuous planning where a new solution can be generated from scratch to accommodate changing requirements. The genetic algorithm maintains a population of solutions where the set of assignments in a solution is represented by a 'chromosome' (for example, a text string). A fitness function takes a chromosome as input and returns a value that represents how good it is. The algorithm chooses two solutions as 'parents' according to their fitness scores and 'mates' them together. Parts of each parent's chromosome are randomly selected and combined to create a 'child' solution with a new chromosome. The child's chromosome is constructed so that sub-orderings in the parents' chromosomes also appear in the child so that the child has the same characteristics as its parents but combined in a new way. Finally, part of the child's chromosome is randomly mutated to introduce some variance in each generation.

In summary, existing research has focused on developing models that represent the ASPP in a realistic way so that solutions to the problem cater for the main constraints and objectives that a real development team aims for when planning their agile sprints. To solve the problem, researchers have focused on either using complete algorithms to find optimal solutions, or using nature-inspired approximation algorithms to find good solutions. This thesis will explore if other highly-popular approximation techniques such as large neighbourhood search, simulated annealing, and tabu search can give good results compared to complete methods based on mathematical programming.